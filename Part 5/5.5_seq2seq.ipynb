{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
    },
    "colab": {
      "name": "5.5_seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "4TPsL7c-nKLm",
        "PRF7_GT9nKLo",
        "y2SKSCVTnKLr",
        "RLR7e81DnKLt",
        "b3WJLF-pnKLx"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ef8p19lsTmb"
      },
      "source": [
        "[구글 코랩(Colab)에서 실행하기](https://colab.research.google.com/github/lovedlim/tensorflow/blob/main/Part%205/5.5_seq2seq.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pV5qtxxcnKLg"
      },
      "source": [
        "## Part5-4-4 시퀀스 투 시퀀스 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct6f7k13nKLi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5aab17a4-15b6-4068-e0ad-fffb4f04cd4a"
      },
      "source": [
        "#Korpora 라이브러리 설치\n",
        "!pip install Korpora"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting Korpora\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/b1/5e563e23f1f705574bbeb55555e0cb95c9813e9396d654cd42709418ab66/Korpora-0.2.0-py3-none-any.whl (57kB)\n",
            "\r\u001b[K     |█████▊                          | 10kB 22.0MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 20kB 29.1MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30kB 22.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 40kB 25.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 51kB 24.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 61kB 6.9MB/s \n",
            "\u001b[?25hCollecting xlrd>=1.2.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/0c/c2a72d51fe56e08a08acc85d13013558a2d793028ae7385448a6ccdfae64/xlrd-2.0.1-py2.py3-none-any.whl (96kB)\n",
            "\r\u001b[K     |███▍                            | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 20kB 34.9MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30kB 33.7MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 40kB 35.6MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 51kB 29.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 61kB 24.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 71kB 21.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 81kB 22.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 92kB 21.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 102kB 8.2MB/s \n",
            "\u001b[?25hCollecting tqdm>=4.46.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/8a/34efae5cf9924328a8f34eeb2fdaae14c011462d9f0e3fcded48e1266d1c/tqdm-4.60.0-py2.py3-none-any.whl (75kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 28.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 20kB 36.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 30kB 40.3MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 40kB 42.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 51kB 43.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 61kB 40.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 71kB 40.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 10.3MB/s \n",
            "\u001b[?25hCollecting dataclasses>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/26/2f/1095cdc2868052dd1e64520f7c0d5c8c550ad297e944e641dbf1ffbb9a5d/dataclasses-0.6-py3-none-any.whl\n",
            "Requirement already satisfied: requests>=2.20.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from Korpora) (1.19.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20.0->Korpora) (3.0.4)\n",
            "Installing collected packages: xlrd, tqdm, dataclasses, Korpora\n",
            "  Found existing installation: xlrd 1.1.0\n",
            "    Uninstalling xlrd-1.1.0:\n",
            "      Successfully uninstalled xlrd-1.1.0\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "Successfully installed Korpora-0.2.0 dataclasses-0.6 tqdm-4.60.0 xlrd-2.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkhO-PWknKLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e56e352b-fc9d-463a-b0f7-324a2aa74541"
      },
      "source": [
        "# 챗봇 라이브러리 불러오기\n",
        "from Korpora import KoreanChatbotKorpus\n",
        "corpus = KoreanChatbotKorpus()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "    Korpora 는 다른 분들이 연구 목적으로 공유해주신 말뭉치들을\n",
            "    손쉽게 다운로드, 사용할 수 있는 기능만을 제공합니다.\n",
            "\n",
            "    말뭉치들을 공유해 주신 분들에게 감사드리며, 각 말뭉치 별 설명과 라이센스를 공유 드립니다.\n",
            "    해당 말뭉치에 대해 자세히 알고 싶으신 분은 아래의 description 을 참고,\n",
            "    해당 말뭉치를 연구/상용의 목적으로 이용하실 때에는 아래의 라이센스를 참고해 주시기 바랍니다.\n",
            "\n",
            "    # Description\n",
            "    Author : songys@github\n",
            "    Repository : https://github.com/songys/Chatbot_data\n",
            "    References :\n",
            "\n",
            "    Chatbot_data_for_Korean v1.0\n",
            "      1. 챗봇 트레이닝용 문답 페어 11,876개\n",
            "      2. 일상다반사 0, 이별(부정) 1, 사랑(긍정) 2로 레이블링\n",
            "    자세한 내용은 위의 repository를 참고하세요.\n",
            "\n",
            "    # License\n",
            "    CC0 1.0 Universal (CC0 1.0) Public Domain Dedication\n",
            "    Details in https://creativecommons.org/publicdomain/zero/1.0/\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[korean_chatbot_data] download ChatbotData.csv: 893kB [00:00, 8.02MB/s]                           \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJCsCjLknKLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "248896b9-8794-4c07-ef80-0e7fb4284eb3"
      },
      "source": [
        "# 챗봇 데이터 샘플 확인\n",
        "print(corpus.get_all_texts()[:5])\n",
        "print(corpus.get_all_pairs()[:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네']\n",
            "('하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "us69ER49nKLk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a57e12-b93a-4c10-f197-50572c949e96"
      },
      "source": [
        "# text와 pair가 쌍으로 이루어짐\n",
        "print(\"Q:\", corpus.train[0].text)\n",
        "print(\"A:\", corpus.train[0].pair)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q: 12시 땡!\n",
            "A: 하루가 또 가네요.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_19hU_nhnKLk"
      },
      "source": [
        "## 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iwYDPUUpRHgc",
        "outputId": "3d72f1d9-e9b9-4391-9443-834d26fa6c2a"
      },
      "source": [
        "# 전체 코퍼스 크기 (11,823개)\n",
        "len(corpus.get_all_texts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11823"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl0H2-RsnKLk"
      },
      "source": [
        "# 2,000개 데이터 셋만 활용 (Google Colab 일 경우 3,000개에서는 메모리 오버되는 현상 발생)\n",
        "texts = []\n",
        "pairs = []\n",
        "for i, (text, pair) in enumerate(zip(corpus.get_all_texts(), corpus.get_all_pairs())):\n",
        "    texts.append(text)\n",
        "    pairs.append(pair)\n",
        "    if i >= 2000: \n",
        "        break "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWuUWju3nKLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "caae88d4-6057-44b8-e4c6-b851997aa1bc"
      },
      "source": [
        "# question와 answer 데이터 확인\n",
        "list(zip(texts, pairs))[1995:2000]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('배 아프다', '약이 필요하면 도움을 받아보세요.'),\n",
              " ('배 터지겠네', '위를 좀 쉬게 해주세요.'),\n",
              " ('배 터지겠다.', '산책 좀 해야겠네여.'),\n",
              " ('배가 너무 고파', '뭐 좀 챙겨드세요.'),\n",
              " ('배가 넘넘 고파', '저도 밥 먹고 싶어요')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkp1mmttnKLl"
      },
      "source": [
        "### 특수문자는 제거합니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnQ5Xb58nKLl"
      },
      "source": [
        "# 데이터 전처리 함수 (정규식(regex expression)활용)\n",
        "import re\n",
        "def clean_sentence(sentence):\n",
        "    # 한글, 숫자를 제외한 모든 문자는 제거합니다.\n",
        "    sentence = re.sub(r'[^0-9ㄱ-ㅎㅏ-ㅣ가-힣 ]',r'', sentence)\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ElslA3SgnKLl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef35026b-42ea-4af0-8f74-20cd0fddcdea"
      },
      "source": [
        "# 전처리 함수 테스트\n",
        "print(clean_sentence('안녕하세요~:)'))\n",
        "print(clean_sentence('텐서플로^@^%#@!'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "안녕하세요\n",
            "텐서플로\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TPsL7c-nKLm"
      },
      "source": [
        "### 한글 형태소 분석기 (Konlpy)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7DnDlZwxnKLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7b9493d-af54-441e-b37f-dcb953929e0d"
      },
      "source": [
        "# konlpy 설치\n",
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting konlpy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/0e/f385566fec837c0b83f216b2da65db9997b35dd675e107752005b7d392b1/konlpy-0.5.2-py2.py3-none-any.whl (19.4MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4MB 155kB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading https://files.pythonhosted.org/packages/44/98/5b86278fbbf250d239ae0ecb724f8572af1c91f4a11edf4d36a206189440/colorama-0.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.19.5)\n",
            "Collecting JPype1>=0.7.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cd/a5/9781e2ef4ca92d09912c4794642c1653aea7607f473e156cf4d423a881a1/JPype1-1.2.1-cp37-cp37m-manylinux2010_x86_64.whl (457kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: tweepy>=3.7.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (3.10.0)\n",
            "Collecting beautifulsoup4==4.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/d4/10f46e5cfac773e22707237bfcd51bbffeaf0a576b0a847ec7ab15bd7ace/beautifulsoup4-4.6.0-py3-none-any.whl (86kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Requirement already satisfied: typing-extensions; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.15.0)\n",
            "Requirement already satisfied: requests[socks]>=2.11.1 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (2.23.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tweepy>=3.7.0->konlpy) (1.3.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (2.10)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6; extra == \"socks\" in /usr/local/lib/python3.7/dist-packages (from requests[socks]>=2.11.1->tweepy>=3.7.0->konlpy) (1.7.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->tweepy>=3.7.0->konlpy) (3.1.0)\n",
            "Installing collected packages: colorama, JPype1, beautifulsoup4, konlpy\n",
            "  Found existing installation: beautifulsoup4 4.6.3\n",
            "    Uninstalling beautifulsoup4-4.6.3:\n",
            "      Successfully uninstalled beautifulsoup4-4.6.3\n",
            "Successfully installed JPype1-1.2.1 beautifulsoup4-4.6.0 colorama-0.4.4 konlpy-0.5.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jeu4-LHHnKLn"
      },
      "source": [
        "# Okt형태소\n",
        "from konlpy.tag import Okt\n",
        "okt = Okt()\n",
        "def process_morph(sentence):\n",
        "    return ' '.join(okt.morphs(sentence))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lv0cn1SDnKLn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "894bacfb-f4ae-44a2-8d55-691de2ebd61d"
      },
      "source": [
        "# 문장 전처리\n",
        "def clean_and_morph(sentence, is_question=True):\n",
        "    # 한글 문장 전처리\n",
        "    sentence = clean_sentence(sentence)\n",
        "    # 형태소 변환\n",
        "    sentence = process_morph(sentence)\n",
        "    # Question 인 경우, Answer인 경우를 분기하여 처리합니다.\n",
        "    if is_question:\n",
        "        return sentence\n",
        "    else:\n",
        "        # START 토큰은 decoder input에 END 토큰은 decoder output에 추가합니다.\n",
        "        return ('<START> ' + sentence, sentence + ' <END>')\n",
        "\n",
        "def preprocess(texts, pairs):\n",
        "    questions = []\n",
        "    answer_in = []\n",
        "    answer_out = []\n",
        "\n",
        "    # 질의에 대한 전처리\n",
        "    for text in texts:\n",
        "        # 전처리와 morph 수행\n",
        "        question = clean_and_morph(text, is_question=True)\n",
        "        questions.append(question)\n",
        "\n",
        "    # 답변에 대한 전처리\n",
        "    for pair in pairs:\n",
        "        # 전처리와 morph 수행\n",
        "        in_, out_ = clean_and_morph(pair, is_question=False)\n",
        "        answer_in.append(in_)\n",
        "        answer_out.append(out_)\n",
        "    \n",
        "    return questions, answer_in, answer_out\n",
        "\n",
        "questions, answer_in, answer_out = preprocess(texts, pairs)\n",
        "print(questions[:2])\n",
        "print(answer_in[:2])\n",
        "print(answer_out[:2])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['12시 땡', '1 지망 학교 떨어졌어']\n",
            "['<START> 하루 가 또 가네요', '<START> 위로 해 드립니다']\n",
            "['하루 가 또 가네요 <END>', '위로 해 드립니다 <END>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sIweX12CnKLo"
      },
      "source": [
        "# 전체 문장을 하나의 리스트로 만들기\n",
        "all_sentences = questions + answer_in + answer_out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRF7_GT9nKLo"
      },
      "source": [
        "## 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CV5E3CZSnKLp"
      },
      "source": [
        "# 라이브러리 불어오기\n",
        "import numpy as np\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# WARNING 무시\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0vA-7xsFnKLp"
      },
      "source": [
        "tokenizer = Tokenizer(filters='', lower=False, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(all_sentences)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g3FBSeF0nKLp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a9c8a91-c1d2-44c7-8a0c-4d11c4a26ed6"
      },
      "source": [
        "# 단어사전 확인\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    print(f'{word}\\t -> \\t{idx}')\n",
        "    if idx > 10:\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<OOV>\t -> \t1\n",
            "<START>\t -> \t2\n",
            "<END>\t -> \t3\n",
            "이\t -> \t4\n",
            "을\t -> \t5\n",
            "거\t -> \t6\n",
            "가\t -> \t7\n",
            "예요\t -> \t8\n",
            "도\t -> \t9\n",
            "해보세요\t -> \t10\n",
            "요\t -> \t11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCA-r61YnKLq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e1a3651-9de9-4ca5-cf47-02ed64102880"
      },
      "source": [
        "# 토큰 갯수 확인\n",
        "len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3604"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L4VTDJxSnKLq"
      },
      "source": [
        "## 치환: 텍스트를 시퀀스로 인코딩 (`texts_to_sequences`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM0c79UnnKLq"
      },
      "source": [
        "# 치환: 텍스트를 시퀀스로 인코딩 (texts_to_sequences)\n",
        "question_sequence = tokenizer.texts_to_sequences(questions)\n",
        "answer_in_sequence = tokenizer.texts_to_sequences(answer_in)\n",
        "answer_out_sequence = tokenizer.texts_to_sequences(answer_out)\n",
        "\n",
        "# 문장의 길이 맞추기 (pad_sequences)\n",
        "MAX_LENGTH = 30\n",
        "question_padded = pad_sequences(question_sequence, \n",
        "                                maxlen=MAX_LENGTH, \n",
        "                                truncating='post', \n",
        "                                padding='post')\n",
        "answer_in_padded = pad_sequences(answer_in_sequence, \n",
        "                                 maxlen=MAX_LENGTH, \n",
        "                                 truncating='post', \n",
        "                                 padding='post')\n",
        "answer_out_padded = pad_sequences(answer_out_sequence, \n",
        "                                  maxlen=MAX_LENGTH, \n",
        "                                  truncating='post', \n",
        "                                  padding='post')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PP6ujoVQnKLr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6511d2dd-fad1-4ad0-965a-ecc933b2ebf3"
      },
      "source": [
        "question_padded.shape, answer_in_padded.shape, answer_out_padded.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2001, 30), (2001, 30), (2001, 30))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2SKSCVTnKLr"
      },
      "source": [
        "## 모델"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqsTUuxXnKLr"
      },
      "source": [
        "# 라이브러리 로드\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zRAumSEnKLr"
      },
      "source": [
        "## 학습용 인코더 (Encoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04jAUIC5nKLr"
      },
      "source": [
        "# 인코더\n",
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, \n",
        "                                   embedding_dim, \n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, return_state=True)\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        return [hidden_state, cell_state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjhVwZNgnKLr"
      },
      "source": [
        "## 학습용 디코더 (Decoder)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4qC3Al0nKLs"
      },
      "source": [
        "# 디코더\n",
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, \n",
        "                                   embedding_dim, \n",
        "                                   input_length=time_steps)\n",
        "        self.dropout = Dropout(0.2)\n",
        "        self.lstm = LSTM(units, \n",
        "                         return_state=True, \n",
        "                         return_sequences=True, \n",
        "                        )\n",
        "        self.dense = Dense(vocab_size, activation='softmax')\n",
        "    \n",
        "    def call(self, inputs, initial_state):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)        \n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CmUTWKknKLs"
      },
      "source": [
        "## 모델 결합"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huOfpcSQnKLs"
      },
      "source": [
        "# 모델 결합\n",
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "        \n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            context_vector = self.encoder(encoder_inputs)\n",
        "            decoder_outputs, _, _ = self.decoder(inputs=decoder_inputs, \n",
        "                                                 initial_state=context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            context_vector = self.encoder(inputs)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "            \n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder(target_seq, \n",
        "                                                                            initial_state=context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), \n",
        "                                         dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "                \n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "                    \n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "                \n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RLR7e81DnKLt"
      },
      "source": [
        "## 단어별 원핫인코딩 적용"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gCNm7zCinKLt"
      },
      "source": [
        "VOCAB_SIZE = len(tokenizer.word_index)+1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfWmtw_DnKLt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6559b357-5ffd-4c37-f6bf-2ccd03e5432b"
      },
      "source": [
        "def convert_to_one_hot(padded):\n",
        "    # 원핫인코딩 초기화\n",
        "    one_hot_vector = np.zeros((len(answer_out_padded), \n",
        "                               MAX_LENGTH, \n",
        "                               VOCAB_SIZE))\n",
        "\n",
        "    # 디코더 목표를 원핫인코딩으로 변환\n",
        "    # 학습시 입력은 인덱스이지만, 출력은 원핫인코딩 형식임\n",
        "    for i, sequence in enumerate(answer_out_padded):\n",
        "        for j, index in enumerate(sequence):\n",
        "            one_hot_vector[i, j, index] = 1\n",
        "\n",
        "    return one_hot_vector\n",
        "\n",
        "answer_in_one_hot = convert_to_one_hot(answer_in_padded)\n",
        "answer_out_one_hot = convert_to_one_hot(answer_out_padded)\n",
        "answer_in_one_hot[0].shape, answer_in_one_hot[0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((30, 3605), (30, 3605))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-upnUy_nKLv"
      },
      "source": [
        "# 변환된 index를 다시 단어로 변환\n",
        "def convert_index_to_text(indexs, end_token): \n",
        "    \n",
        "    sentence = ''\n",
        "    \n",
        "    # 모든 문장에 대해서 반복\n",
        "    for index in indexs:\n",
        "        if index == end_token:\n",
        "            # 끝 단어이므로 예측 중비\n",
        "            break;\n",
        "        # 사전에 존재하는 단어의 경우 단어 추가\n",
        "        if index > 0 and tokenizer.index_word[index] is not None:\n",
        "            sentence += tokenizer.index_word[index]\n",
        "        else:\n",
        "        # 사전에 없는 인덱스면 빈 문자열 추가\n",
        "            sentence += ''\n",
        "            \n",
        "        # 빈칸 추가\n",
        "        sentence += ' '\n",
        "    return sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYvjxOEJnKLv"
      },
      "source": [
        "## 학습 (Training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_bcNTBJSnKLv"
      },
      "source": [
        "# 하이퍼 파라미터 정의\n",
        "BUFFER_SIZE = 1000\n",
        "BATCH_SIZE = 16\n",
        "EMBEDDING_DIM = 100\n",
        "TIME_STEPS = MAX_LENGTH\n",
        "START_TOKEN = tokenizer.word_index['<START>']\n",
        "END_TOKEN = tokenizer.word_index['<END>']\n",
        "\n",
        "UNITS = 128\n",
        "\n",
        "VOCAB_SIZE = len(tokenizer.word_index)+1\n",
        "DATA_LENGTH = len(questions)\n",
        "SAMPLE_SIZE = 3\n",
        "NUM_EPOCHS = 20"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHPtTUEmnKLv"
      },
      "source": [
        "# 체크포인트 생성\n",
        "checkpoint_path = 'model/seq2seq-chatbot-no-attention-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True, \n",
        "                             monitor='loss', \n",
        "                             verbose=1\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CD0k5Yv-tL9z"
      },
      "source": [
        "# seq2seq\n",
        "seq2seq = Seq2Seq(UNITS, \n",
        "                  VOCAB_SIZE, \n",
        "                  EMBEDDING_DIM, \n",
        "                  TIME_STEPS, \n",
        "                  START_TOKEN, \n",
        "                  END_TOKEN)\n",
        "\n",
        "seq2seq.compile(optimizer='adam', \n",
        "                loss='categorical_crossentropy', \n",
        "                metrics=['acc'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfbT_Eo5nKLw"
      },
      "source": [
        "def make_prediction(model, question_inputs):\n",
        "    results = model(inputs=question_inputs, training=False)\n",
        "    # 변환된 인덱스를 문장으로 변환\n",
        "    results = np.asarray(results).reshape(-1)\n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6pZYUJ6nKLw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9fab3b2-037a-4cae-9036-d5e6909b41bf"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq.fit([question_padded, answer_in_padded],\n",
        "                answer_out_one_hot,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "        \n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "        \n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 37s 20ms/step - loss: 4.2731 - acc: 0.7656\n",
            "\n",
            "Epoch 00001: loss improved from inf to 2.41994, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 1.2430 - acc: 0.8107\n",
            "\n",
            "Epoch 00002: loss improved from 2.41994 to 1.20443, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 1.1143 - acc: 0.8356\n",
            "\n",
            "Epoch 00003: loss improved from 1.20443 to 1.10736, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 1.0663 - acc: 0.8382\n",
            "\n",
            "Epoch 00004: loss improved from 1.10736 to 1.05717, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 1.0337 - acc: 0.8389\n",
            "\n",
            "Epoch 00005: loss improved from 1.05717 to 1.02229, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.9659 - acc: 0.8477\n",
            "\n",
            "Epoch 00006: loss improved from 1.02229 to 0.99591, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.9711 - acc: 0.8445\n",
            "\n",
            "Epoch 00007: loss improved from 0.99591 to 0.97262, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.9432 - acc: 0.8473\n",
            "\n",
            "Epoch 00008: loss improved from 0.97262 to 0.95224, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.9117 - acc: 0.8494\n",
            "\n",
            "Epoch 00009: loss improved from 0.95224 to 0.93370, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.9057 - acc: 0.8496\n",
            "\n",
            "Epoch 00010: loss improved from 0.93370 to 0.91680, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 과일 먹고 자야지\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 마음 을 정리 하고 있는데 뜻밖 의 남자 가 카톡 을 했네\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "Q: 단발 해볼까\n",
            "A: 저 을 더 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.9014 - acc: 0.8495\n",
            "\n",
            "Epoch 00001: loss improved from 0.91680 to 0.90141, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.8868 - acc: 0.8507\n",
            "\n",
            "Epoch 00002: loss improved from 0.90141 to 0.88683, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.8719 - acc: 0.8517\n",
            "\n",
            "Epoch 00003: loss improved from 0.88683 to 0.87189, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.8560 - acc: 0.8528\n",
            "\n",
            "Epoch 00004: loss improved from 0.87189 to 0.85604, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.8392 - acc: 0.8547\n",
            "\n",
            "Epoch 00005: loss improved from 0.85604 to 0.83922, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.8211 - acc: 0.8566\n",
            "\n",
            "Epoch 00006: loss improved from 0.83922 to 0.82107, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.8013 - acc: 0.8593\n",
            "\n",
            "Epoch 00007: loss improved from 0.82107 to 0.80125, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.7817 - acc: 0.8625\n",
            "\n",
            "Epoch 00008: loss improved from 0.80125 to 0.78172, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.7610 - acc: 0.8654\n",
            "\n",
            "Epoch 00009: loss improved from 0.78172 to 0.76100, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.7415 - acc: 0.8677\n",
            "\n",
            "Epoch 00010: loss improved from 0.76100 to 0.74150, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 농담 처럼 진담 하는 사람\n",
            "A: 저 이 있으면 있어요 \n",
            "\n",
            "\n",
            "Q: 농구 해야지\n",
            "A: 저 도 마음 을 주변 사람 이 있어요 \n",
            "\n",
            "\n",
            "Q: 단체 생활 해야 되는거 싫다\n",
            "A: 저 도 마음 을 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.7217 - acc: 0.8706\n",
            "\n",
            "Epoch 00001: loss improved from 0.74150 to 0.72168, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.7033 - acc: 0.8725\n",
            "\n",
            "Epoch 00002: loss improved from 0.72168 to 0.70327, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6840 - acc: 0.8748\n",
            "\n",
            "Epoch 00003: loss improved from 0.70327 to 0.68404, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6661 - acc: 0.8771\n",
            "\n",
            "Epoch 00004: loss improved from 0.68404 to 0.66611, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6485 - acc: 0.8794\n",
            "\n",
            "Epoch 00005: loss improved from 0.66611 to 0.64847, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.6312 - acc: 0.8825\n",
            "\n",
            "Epoch 00006: loss improved from 0.64847 to 0.63117, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.6160 - acc: 0.8847\n",
            "\n",
            "Epoch 00007: loss improved from 0.63117 to 0.61595, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.6007 - acc: 0.8877\n",
            "\n",
            "Epoch 00008: loss improved from 0.61595 to 0.60067, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.5828 - acc: 0.8903\n",
            "\n",
            "Epoch 00009: loss improved from 0.60067 to 0.58279, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.5663 - acc: 0.8934\n",
            "\n",
            "Epoch 00010: loss improved from 0.58279 to 0.56631, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 내일 날씨 좋을까\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 나 한테 만 왜 이런 일이 일어날까\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 내 가 좋아하는 사람 이 행복했으면 좋겠다\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.5514 - acc: 0.8968\n",
            "\n",
            "Epoch 00001: loss improved from 0.56631 to 0.55139, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.5351 - acc: 0.9001\n",
            "\n",
            "Epoch 00002: loss improved from 0.55139 to 0.53507, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.5204 - acc: 0.9033\n",
            "\n",
            "Epoch 00003: loss improved from 0.53507 to 0.52036, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.5054 - acc: 0.9061\n",
            "\n",
            "Epoch 00004: loss improved from 0.52036 to 0.50540, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.4922 - acc: 0.9091\n",
            "\n",
            "Epoch 00005: loss improved from 0.50540 to 0.49219, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.4784 - acc: 0.9121\n",
            "\n",
            "Epoch 00006: loss improved from 0.49219 to 0.47837, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.4661 - acc: 0.9146\n",
            "\n",
            "Epoch 00007: loss improved from 0.47837 to 0.46609, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.4534 - acc: 0.9174\n",
            "\n",
            "Epoch 00008: loss improved from 0.46609 to 0.45335, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.4426 - acc: 0.9186\n",
            "\n",
            "Epoch 00009: loss improved from 0.45335 to 0.44255, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.4308 - acc: 0.9218\n",
            "\n",
            "Epoch 00010: loss improved from 0.44255 to 0.43076, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 드디어 개강 이다\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 면도 하는 게 낫겠지\n",
            "A: 잘 하고 있어요 \n",
            "\n",
            "\n",
            "Q: 내년 에는 더 행복해질 려고 이렇게 힘든 가봅니다\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.4204 - acc: 0.9248\n",
            "\n",
            "Epoch 00001: loss improved from 0.43076 to 0.42041, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.4100 - acc: 0.9260\n",
            "\n",
            "Epoch 00002: loss improved from 0.42041 to 0.40997, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.4004 - acc: 0.9275\n",
            "\n",
            "Epoch 00003: loss improved from 0.40997 to 0.40039, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.3912 - acc: 0.9298\n",
            "\n",
            "Epoch 00004: loss improved from 0.40039 to 0.39117, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3834 - acc: 0.9310\n",
            "\n",
            "Epoch 00005: loss improved from 0.39117 to 0.38344, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.3748 - acc: 0.9325\n",
            "\n",
            "Epoch 00006: loss improved from 0.38344 to 0.37483, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3679 - acc: 0.9337\n",
            "\n",
            "Epoch 00007: loss improved from 0.37483 to 0.36794, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.3601 - acc: 0.9357\n",
            "\n",
            "Epoch 00008: loss improved from 0.36794 to 0.36010, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.3533 - acc: 0.9363\n",
            "\n",
            "Epoch 00009: loss improved from 0.36010 to 0.35332, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.3462 - acc: 0.9378\n",
            "\n",
            "Epoch 00010: loss improved from 0.35332 to 0.34615, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 내 인생 의 주인공 은 나야\n",
            "A: 잘 하실 거 예요 \n",
            "\n",
            "\n",
            "Q: 그냥 혼자 있는게 좋아\n",
            "A: 잘 하실 거 예요 \n",
            "\n",
            "\n",
            "Q: 거짓말 이 거짓말 을 낳아\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3408 - acc: 0.9391\n",
            "\n",
            "Epoch 00001: loss improved from 0.34615 to 0.34085, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.3365 - acc: 0.9393\n",
            "\n",
            "Epoch 00002: loss improved from 0.34085 to 0.33646, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3279 - acc: 0.9411\n",
            "\n",
            "Epoch 00003: loss improved from 0.33646 to 0.32792, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3234 - acc: 0.9417\n",
            "\n",
            "Epoch 00004: loss improved from 0.32792 to 0.32337, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.3179 - acc: 0.9433\n",
            "\n",
            "Epoch 00005: loss improved from 0.32337 to 0.31794, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.3127 - acc: 0.9438\n",
            "\n",
            "Epoch 00006: loss improved from 0.31794 to 0.31272, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3083 - acc: 0.9439\n",
            "\n",
            "Epoch 00007: loss improved from 0.31272 to 0.30828, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.3047 - acc: 0.9448\n",
            "\n",
            "Epoch 00008: loss improved from 0.30828 to 0.30473, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.3000 - acc: 0.9456\n",
            "\n",
            "Epoch 00009: loss improved from 0.30473 to 0.29998, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2949 - acc: 0.9463\n",
            "\n",
            "Epoch 00010: loss improved from 0.29998 to 0.29485, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 냉방 비 장난 아님\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 너 또 뭐 할 줄 알 아\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 꽃다발 받았어\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.2912 - acc: 0.9469\n",
            "\n",
            "Epoch 00001: loss improved from 0.29485 to 0.29118, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2875 - acc: 0.9470\n",
            "\n",
            "Epoch 00002: loss improved from 0.29118 to 0.28755, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2846 - acc: 0.9477\n",
            "\n",
            "Epoch 00003: loss improved from 0.28755 to 0.28462, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2806 - acc: 0.9486\n",
            "\n",
            "Epoch 00004: loss improved from 0.28462 to 0.28059, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2773 - acc: 0.9490\n",
            "\n",
            "Epoch 00005: loss improved from 0.28059 to 0.27733, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2741 - acc: 0.9495\n",
            "\n",
            "Epoch 00006: loss improved from 0.27733 to 0.27407, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2712 - acc: 0.9502\n",
            "\n",
            "Epoch 00007: loss improved from 0.27407 to 0.27123, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2682 - acc: 0.9505\n",
            "\n",
            "Epoch 00008: loss improved from 0.27123 to 0.26815, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2666 - acc: 0.9508\n",
            "\n",
            "Epoch 00009: loss improved from 0.26815 to 0.26658, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2642 - acc: 0.9508\n",
            "\n",
            "Epoch 00010: loss improved from 0.26658 to 0.26421, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 발목 접 질렀어\n",
            "A: 지금 도 충분해요 \n",
            "\n",
            "\n",
            "Q: 교양 이 전공 보다 재미있어\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 면도기 사야 되는데\n",
            "A: 잘 하고 있어요 당당해지세요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.2630 - acc: 0.9508\n",
            "\n",
            "Epoch 00001: loss improved from 0.26421 to 0.26297, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2589 - acc: 0.9520\n",
            "\n",
            "Epoch 00002: loss improved from 0.26297 to 0.25887, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2568 - acc: 0.9523\n",
            "\n",
            "Epoch 00003: loss improved from 0.25887 to 0.25681, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2536 - acc: 0.9522\n",
            "\n",
            "Epoch 00004: loss improved from 0.25681 to 0.25365, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2517 - acc: 0.9533\n",
            "\n",
            "Epoch 00005: loss improved from 0.25365 to 0.25167, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2501 - acc: 0.9535\n",
            "\n",
            "Epoch 00006: loss improved from 0.25167 to 0.25009, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2470 - acc: 0.9536\n",
            "\n",
            "Epoch 00007: loss improved from 0.25009 to 0.24702, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2459 - acc: 0.9536\n",
            "\n",
            "Epoch 00008: loss improved from 0.24702 to 0.24592, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2439 - acc: 0.9539\n",
            "\n",
            "Epoch 00009: loss improved from 0.24592 to 0.24390, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2430 - acc: 0.9542\n",
            "\n",
            "Epoch 00010: loss improved from 0.24390 to 0.24301, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 무기력증 어떻게 극복 하지\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 기분 전환 하고 싶어\n",
            "A: 제 가 놀아 드리고 싶네요 \n",
            "\n",
            "\n",
            "Q: 다이어트 해야 되는데\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2410 - acc: 0.9546\n",
            "\n",
            "Epoch 00001: loss improved from 0.24301 to 0.24103, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2428 - acc: 0.9545\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.24103\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2385 - acc: 0.9543\n",
            "\n",
            "Epoch 00003: loss improved from 0.24103 to 0.23846, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2367 - acc: 0.9549\n",
            "\n",
            "Epoch 00004: loss improved from 0.23846 to 0.23670, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2346 - acc: 0.9551\n",
            "\n",
            "Epoch 00005: loss improved from 0.23670 to 0.23459, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2335 - acc: 0.9555\n",
            "\n",
            "Epoch 00006: loss improved from 0.23459 to 0.23345, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2315 - acc: 0.9557\n",
            "\n",
            "Epoch 00007: loss improved from 0.23345 to 0.23148, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2297 - acc: 0.9558\n",
            "\n",
            "Epoch 00008: loss improved from 0.23148 to 0.22973, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2283 - acc: 0.9562\n",
            "\n",
            "Epoch 00009: loss improved from 0.22973 to 0.22829, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2274 - acc: 0.9563\n",
            "\n",
            "Epoch 00010: loss improved from 0.22829 to 0.22739, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 괴물 이 되어 가는 느낌 이 들어\n",
            "A: 제 가 따라가려면 멀었네요 \n",
            "\n",
            "\n",
            "Q: 결혼 하는데 돈 얼마나 들까\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 곱창 생각나\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.2268 - acc: 0.9569\n",
            "\n",
            "Epoch 00001: loss improved from 0.22739 to 0.22683, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2259 - acc: 0.9568\n",
            "\n",
            "Epoch 00002: loss improved from 0.22683 to 0.22588, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2250 - acc: 0.9564\n",
            "\n",
            "Epoch 00003: loss improved from 0.22588 to 0.22495, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2237 - acc: 0.9567\n",
            "\n",
            "Epoch 00004: loss improved from 0.22495 to 0.22370, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.2223 - acc: 0.9572\n",
            "\n",
            "Epoch 00005: loss improved from 0.22370 to 0.22233, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2228 - acc: 0.9573\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.22233\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2230 - acc: 0.9569\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.22233\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2204 - acc: 0.9574\n",
            "\n",
            "Epoch 00008: loss improved from 0.22233 to 0.22036, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.2187 - acc: 0.9576\n",
            "\n",
            "Epoch 00009: loss improved from 0.22036 to 0.21869, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2179 - acc: 0.9574\n",
            "\n",
            "Epoch 00010: loss improved from 0.21869 to 0.21793, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 무슨 매주 결혼식 이야\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 면도 귀 찬 하\n",
            "A: 당신 이 요 \n",
            "\n",
            "\n",
            "Q: 남편 이 회식 하면 늦게 들어와\n",
            "A: 저 는 마음 을 이어주는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.2167 - acc: 0.9580\n",
            "\n",
            "Epoch 00001: loss improved from 0.21793 to 0.21672, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2160 - acc: 0.9579\n",
            "\n",
            "Epoch 00002: loss improved from 0.21672 to 0.21604, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2153 - acc: 0.9582\n",
            "\n",
            "Epoch 00003: loss improved from 0.21604 to 0.21526, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2143 - acc: 0.9584\n",
            "\n",
            "Epoch 00004: loss improved from 0.21526 to 0.21433, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2134 - acc: 0.9579\n",
            "\n",
            "Epoch 00005: loss improved from 0.21433 to 0.21337, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2121 - acc: 0.9585\n",
            "\n",
            "Epoch 00006: loss improved from 0.21337 to 0.21207, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2114 - acc: 0.9586\n",
            "\n",
            "Epoch 00007: loss improved from 0.21207 to 0.21143, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2104 - acc: 0.9590\n",
            "\n",
            "Epoch 00008: loss improved from 0.21143 to 0.21036, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2092 - acc: 0.9594\n",
            "\n",
            "Epoch 00009: loss improved from 0.21036 to 0.20916, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.2122 - acc: 0.9583\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.20916\n",
            "Q: 내 가 말 하면 왜 비난 만 할까\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "Q: 대학 가면 좋겠지\n",
            "A: 잘 할 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 내 가 제 정신 이 아니다\n",
            "A: 저 도 듣고 싶네요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.2084 - acc: 0.9592\n",
            "\n",
            "Epoch 00001: loss improved from 0.20916 to 0.20839, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2076 - acc: 0.9587\n",
            "\n",
            "Epoch 00002: loss improved from 0.20839 to 0.20764, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2056 - acc: 0.9591\n",
            "\n",
            "Epoch 00003: loss improved from 0.20764 to 0.20562, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.2050 - acc: 0.9595\n",
            "\n",
            "Epoch 00004: loss improved from 0.20562 to 0.20502, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2040 - acc: 0.9593\n",
            "\n",
            "Epoch 00005: loss improved from 0.20502 to 0.20398, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.2024 - acc: 0.9596\n",
            "\n",
            "Epoch 00006: loss improved from 0.20398 to 0.20243, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.2008 - acc: 0.9601\n",
            "\n",
            "Epoch 00007: loss improved from 0.20243 to 0.20083, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.2001 - acc: 0.9597\n",
            "\n",
            "Epoch 00008: loss improved from 0.20083 to 0.20006, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1997 - acc: 0.9596\n",
            "\n",
            "Epoch 00009: loss improved from 0.20006 to 0.19974, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1987 - acc: 0.9601\n",
            "\n",
            "Epoch 00010: loss improved from 0.19974 to 0.19873, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 다른 사람 의 시선 이 너무 신경 쓰여\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 먼지 가 너무 많아\n",
            "A: 지금 도 멋져요 \n",
            "\n",
            "\n",
            "Q: 모르는 사람 만나는거 너무 스트레스 야\n",
            "A: 더 좋은 곳 에서 살 수 있을 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1980 - acc: 0.9597\n",
            "\n",
            "Epoch 00001: loss improved from 0.19873 to 0.19802, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1958 - acc: 0.9600\n",
            "\n",
            "Epoch 00002: loss improved from 0.19802 to 0.19577, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1997 - acc: 0.9594\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.19577\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1969 - acc: 0.9599\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.19577\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1947 - acc: 0.9600\n",
            "\n",
            "Epoch 00005: loss improved from 0.19577 to 0.19470, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1938 - acc: 0.9601\n",
            "\n",
            "Epoch 00006: loss improved from 0.19470 to 0.19376, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1919 - acc: 0.9605\n",
            "\n",
            "Epoch 00007: loss improved from 0.19376 to 0.19193, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1910 - acc: 0.9604\n",
            "\n",
            "Epoch 00008: loss improved from 0.19193 to 0.19098, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1897 - acc: 0.9605\n",
            "\n",
            "Epoch 00009: loss improved from 0.19098 to 0.18970, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1896 - acc: 0.9605\n",
            "\n",
            "Epoch 00010: loss improved from 0.18970 to 0.18963, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 내 외모 맘 에 안 들어\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 놀러 가고싶다\n",
            "A: 잘 하는 것 도 많아요 \n",
            "\n",
            "\n",
            "Q: 가스 불 켜고 나갔어\n",
            "A: 더 좋은 기회 가 올 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1885 - acc: 0.9609\n",
            "\n",
            "Epoch 00001: loss improved from 0.18963 to 0.18853, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1870 - acc: 0.9606\n",
            "\n",
            "Epoch 00002: loss improved from 0.18853 to 0.18702, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1866 - acc: 0.9610\n",
            "\n",
            "Epoch 00003: loss improved from 0.18702 to 0.18660, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1853 - acc: 0.9611\n",
            "\n",
            "Epoch 00004: loss improved from 0.18660 to 0.18532, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1858 - acc: 0.9610\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.18532\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1841 - acc: 0.9611\n",
            "\n",
            "Epoch 00006: loss improved from 0.18532 to 0.18414, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1840 - acc: 0.9611\n",
            "\n",
            "Epoch 00007: loss improved from 0.18414 to 0.18401, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1828 - acc: 0.9614\n",
            "\n",
            "Epoch 00008: loss improved from 0.18401 to 0.18279, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1822 - acc: 0.9613\n",
            "\n",
            "Epoch 00009: loss improved from 0.18279 to 0.18220, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1821 - acc: 0.9613\n",
            "\n",
            "Epoch 00010: loss improved from 0.18220 to 0.18214, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 노래 잘 하는 사람 부러워\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 나 만 우스워질 거 같아\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 나를 미소 짓게 만든 너\n",
            "A: 다 잘 될 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1811 - acc: 0.9615\n",
            "\n",
            "Epoch 00001: loss improved from 0.18214 to 0.18114, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1814 - acc: 0.9610\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.18114\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1803 - acc: 0.9617\n",
            "\n",
            "Epoch 00003: loss improved from 0.18114 to 0.18034, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1823 - acc: 0.9606\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.18034\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1898 - acc: 0.9596\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.18034\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1847 - acc: 0.9605\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.18034\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1802 - acc: 0.9612\n",
            "\n",
            "Epoch 00007: loss improved from 0.18034 to 0.18016, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1784 - acc: 0.9616\n",
            "\n",
            "Epoch 00008: loss improved from 0.18016 to 0.17844, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1779 - acc: 0.9614\n",
            "\n",
            "Epoch 00009: loss improved from 0.17844 to 0.17790, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1781 - acc: 0.9615\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.17790\n",
            "Q: 말 조심해야지\n",
            "A: 제 가 있잖아요 \n",
            "\n",
            "\n",
            "Q: 뭐 하면서 노 는 게 좋을까\n",
            "A: 저 랑 놀아요 \n",
            "\n",
            "\n",
            "Q: 마라톤 출전 할까\n",
            "A: 맛있게 드세요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1767 - acc: 0.9619\n",
            "\n",
            "Epoch 00001: loss improved from 0.17790 to 0.17669, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1756 - acc: 0.9620\n",
            "\n",
            "Epoch 00002: loss improved from 0.17669 to 0.17560, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1742 - acc: 0.9622\n",
            "\n",
            "Epoch 00003: loss improved from 0.17560 to 0.17417, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1727 - acc: 0.9624\n",
            "\n",
            "Epoch 00004: loss improved from 0.17417 to 0.17265, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1728 - acc: 0.9620\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.17265\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1777 - acc: 0.9618\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.17265\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1731 - acc: 0.9620\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.17265\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1709 - acc: 0.9623\n",
            "\n",
            "Epoch 00008: loss improved from 0.17265 to 0.17089, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1711 - acc: 0.9622\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.17089\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1701 - acc: 0.9622\n",
            "\n",
            "Epoch 00010: loss improved from 0.17089 to 0.17013, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 나 미팅 한다\n",
            "A: 저 도 보고 싶어요 \n",
            "\n",
            "\n",
            "Q: 나 정신차리게 말 해줘\n",
            "A: 그런 척 하는 걸 수도 있어요 \n",
            "\n",
            "\n",
            "Q: 다리 가 퉁퉁 부었어\n",
            "A: 바빠도 힘내세요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1686 - acc: 0.9632\n",
            "\n",
            "Epoch 00001: loss improved from 0.17013 to 0.16857, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1665 - acc: 0.9632\n",
            "\n",
            "Epoch 00002: loss improved from 0.16857 to 0.16655, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1668 - acc: 0.9628\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.16655\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1666 - acc: 0.9631\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.16655\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1664 - acc: 0.9627\n",
            "\n",
            "Epoch 00005: loss improved from 0.16655 to 0.16645, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1635 - acc: 0.9631\n",
            "\n",
            "Epoch 00006: loss improved from 0.16645 to 0.16350, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1637 - acc: 0.9631\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.16350\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1631 - acc: 0.9628\n",
            "\n",
            "Epoch 00008: loss improved from 0.16350 to 0.16312, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1632 - acc: 0.9630\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.16312\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1614 - acc: 0.9632\n",
            "\n",
            "Epoch 00010: loss improved from 0.16312 to 0.16140, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Q: 말 조심해야지\n",
            "A: 눈 을 감고 명상 을 해보세요 \n",
            "\n",
            "\n",
            "Q: 맥주 한 잔 어때\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 나 한테 냄새 날까\n",
            "A: 너무 무리하면 지쳐요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.1622 - acc: 0.9632\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.16140\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1637 - acc: 0.9627\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.16140\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1618 - acc: 0.9629\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.16140\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1614 - acc: 0.9632\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.16140\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1612 - acc: 0.9635\n",
            "\n",
            "Epoch 00005: loss improved from 0.16140 to 0.16116, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1600 - acc: 0.9632\n",
            "\n",
            "Epoch 00006: loss improved from 0.16116 to 0.15997, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1573 - acc: 0.9634\n",
            "\n",
            "Epoch 00007: loss improved from 0.15997 to 0.15731, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1598 - acc: 0.9636\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.15731\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1626 - acc: 0.9633\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.15731\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1630 - acc: 0.9630\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.15731\n",
            "Q: 내 가 이상한가\n",
            "A: 저 도 궁금하네요 \n",
            "\n",
            "\n",
            "Q: 그냥 택시 타야지\n",
            "A: 남자 도 좋은 것 만은 아니예요 \n",
            "\n",
            "\n",
            "Q: 남편 이 육아 를 안해\n",
            "A: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.1578 - acc: 0.9633\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.15731\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1540 - acc: 0.9646\n",
            "\n",
            "Epoch 00002: loss improved from 0.15731 to 0.15405, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1532 - acc: 0.9638\n",
            "\n",
            "Epoch 00003: loss improved from 0.15405 to 0.15318, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1518 - acc: 0.9641\n",
            "\n",
            "Epoch 00004: loss improved from 0.15318 to 0.15184, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1540 - acc: 0.9641\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.15184\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1510 - acc: 0.9643\n",
            "\n",
            "Epoch 00006: loss improved from 0.15184 to 0.15100, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1489 - acc: 0.9646\n",
            "\n",
            "Epoch 00007: loss improved from 0.15100 to 0.14892, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1472 - acc: 0.9648\n",
            "\n",
            "Epoch 00008: loss improved from 0.14892 to 0.14717, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1456 - acc: 0.9652\n",
            "\n",
            "Epoch 00009: loss improved from 0.14717 to 0.14556, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1466 - acc: 0.9648\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.14556\n",
            "Q: 머리 가 지 끈거려\n",
            "A: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "\n",
            "\n",
            "Q: 날씨 풀렸다\n",
            "A: 자신 의 감정 을 주변 사람 들 에게 터 놓고 이야기 해보세요 \n",
            "\n",
            "\n",
            "Q: 방학 이 필요해\n",
            "A: 혼자 를 즐기세요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1443 - acc: 0.9654\n",
            "\n",
            "Epoch 00001: loss improved from 0.14556 to 0.14426, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1452 - acc: 0.9654\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.14426\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1448 - acc: 0.9650\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.14426\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1448 - acc: 0.9654\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.14426\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1450 - acc: 0.9655\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.14426\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1426 - acc: 0.9661\n",
            "\n",
            "Epoch 00006: loss improved from 0.14426 to 0.14257, saving model to model/seq2seq-chatbot-no-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1431 - acc: 0.9658\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.14257\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.2189 - acc: 0.9556\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.14257\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1872 - acc: 0.9596\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.14257\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1717 - acc: 0.9611\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.14257\n",
            "Q: 나 같이 예쁜 애 를 왜 갈구지\n",
            "A: 그러게 말 이에요 \n",
            "\n",
            "\n",
            "Q: 노래방 가면 어색할까\n",
            "A: 저 도 듣고 싶어요 \n",
            "\n",
            "\n",
            "Q: 남자친구 랑 종교 문제 로 다툼\n",
            "A: 당신 의 능력 을 잘 찾아보세요 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3WJLF-pnKLx"
      },
      "source": [
        "## 예측"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBEtnYCjnKLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ba14b67-9366-4c27-af6c-98df3a206567"
      },
      "source": [
        "# 자연어 (질문 입력) 대한 전처리 함수\n",
        "def make_question(sentence):\n",
        "    sentence = clean_and_morph(sentence)\n",
        "    question_sequence = tokenizer.texts_to_sequences([sentence])\n",
        "    question_padded = pad_sequences(question_sequence, maxlen=MAX_LENGTH, truncating='post', padding='post')\n",
        "    return question_padded\n",
        "\n",
        "make_question('오늘 날씨 어때?')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[124, 170, 347,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
              "          0,   0,   0,   0]], dtype=int32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QMfD3cAInKLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7fb8347-d285-40c0-f9ae-7e1a420195e2"
      },
      "source": [
        "# 챗봇\n",
        "def run_chatbot(question):\n",
        "    question_inputs = make_question(question)\n",
        "    results = make_prediction(seq2seq, question_inputs)\n",
        "    results = convert_index_to_text(results, END_TOKEN)\n",
        "    return results\n",
        "\n",
        "# 챗봇 실행\n",
        "while True:\n",
        "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
        "    if user_input == 'q':\n",
        "        break\n",
        "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<< 말을 걸어 보세요!\n",
            "오늘 날씨 어때?\n",
            ">> 챗봇 응답: 살짝 감정 을 흘려 보세요 \n",
            "<< 말을 걸어 보세요!\n",
            "q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kA5wSZ47cpS4"
      },
      "source": [
        "## 시퀀스 투 시퀀스 with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-afhtbmzcsF_"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Embedding, LSTM, Dense, Dropout, Attention\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import TensorBoard, ModelCheckpoint\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNdGR41XdzS-"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, \n",
        "                                   embedding_dim, \n",
        "                                   input_length=time_steps, \n",
        "                                   name='Embedding')\n",
        "        self.dropout = Dropout(0.2, name='Dropout')\n",
        "        # (attention) return_sequences=True 추가\n",
        "        self.lstm = LSTM(units, \n",
        "                         return_state=True, \n",
        "                         return_sequences=True, \n",
        "                         name='LSTM')\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        x = self.embedding(inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x)\n",
        "        # (attention) x return 추가\n",
        "        return x, [hidden_state, cell_state]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izIdy4qhd05u"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = Embedding(vocab_size, \n",
        "                                   embedding_dim, \n",
        "                                   input_length=time_steps, \n",
        "                                   name='Embedding')\n",
        "        self.dropout = Dropout(0.2, name='Dropout')\n",
        "        self.lstm = LSTM(units, \n",
        "                         return_state=True, \n",
        "                         return_sequences=True, \n",
        "                         name='LSTM'\n",
        "                        )\n",
        "        self.attention = Attention(name='Attention')\n",
        "        self.dense = Dense(vocab_size, \n",
        "                           activation='softmax', \n",
        "                           name='Dense')\n",
        "    \n",
        "    def call(self, inputs, initial_state):\n",
        "        # (attention) encoder_inputs 추가\n",
        "        encoder_inputs, decoder_inputs = inputs\n",
        "        x = self.embedding(decoder_inputs)\n",
        "        x = self.dropout(x)\n",
        "        x, hidden_state, cell_state = self.lstm(x, initial_state=initial_state)\n",
        "        \n",
        "        # (attention) key_value, attention_matrix 추가\n",
        "        # 이전 hidden_state의 값을 concat으로 만들어 vector를 생성합니다.        \n",
        "        key_value = tf.concat([initial_state[0][:, tf.newaxis, :], \n",
        "                               x[:, :-1, :]], axis=1)        \n",
        "        # 이전 hidden_state의 값을 concat으로 만든 vector와 encoder에서 나온 \n",
        "        # 출력 값들로 attention을 구합니다.\n",
        "        attention_matrix = self.attention([key_value, encoder_inputs])\n",
        "        # 위에서 구한 attention_matrix와 decoder의 출력 값을 concat 합니다.\n",
        "        x = tf.concat([x, attention_matrix], axis=-1)\n",
        "        \n",
        "        x = self.dense(x)\n",
        "        return x, hidden_state, cell_state"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IufYFei0d3VM"
      },
      "source": [
        "class Seq2Seq(tf.keras.Model):\n",
        "    def __init__(self, units, vocab_size, embedding_dim, time_steps, start_token, end_token):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.start_token = start_token\n",
        "        self.end_token = end_token\n",
        "        self.time_steps = time_steps\n",
        "        \n",
        "        self.encoder = Encoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        self.decoder = Decoder(units, vocab_size, embedding_dim, time_steps)\n",
        "        \n",
        "        \n",
        "    def call(self, inputs, training=True):\n",
        "        if training:\n",
        "            encoder_inputs, decoder_inputs = inputs\n",
        "            # (attention) encoder 출력 값 수정\n",
        "            encoder_outputs, context_vector = self.encoder(encoder_inputs)\n",
        "            # (attention) decoder 입력 값 수정\n",
        "            decoder_outputs, _, _ = self.decoder((encoder_outputs, decoder_inputs), \n",
        "                                                 initial_state=context_vector)\n",
        "            return decoder_outputs\n",
        "        else:\n",
        "            x = inputs\n",
        "            # (attention) encoder 출력 값 수정\n",
        "            encoder_outputs, context_vector = self.encoder(x)\n",
        "            target_seq = tf.constant([[self.start_token]], dtype=tf.float32)\n",
        "            results = tf.TensorArray(tf.int32, self.time_steps)\n",
        "            \n",
        "            for i in tf.range(self.time_steps):\n",
        "                decoder_output, decoder_hidden, decoder_cell = self.decoder((encoder_outputs, target_seq), \n",
        "                                                                            initial_state=context_vector)\n",
        "                decoder_output = tf.cast(tf.argmax(decoder_output, axis=-1), dtype=tf.int32)\n",
        "                decoder_output = tf.reshape(decoder_output, shape=(1, 1))\n",
        "                results = results.write(i, decoder_output)\n",
        "                \n",
        "                if decoder_output == self.end_token:\n",
        "                    break\n",
        "                    \n",
        "                target_seq = decoder_output\n",
        "                context_vector = [decoder_hidden, decoder_cell]\n",
        "                \n",
        "            return tf.reshape(results.stack(), shape=(1, self.time_steps))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2167U8_d6Tz"
      },
      "source": [
        "checkpoint_path = 'model/seq2seq-attention-checkpoint.ckpt'\n",
        "checkpoint = ModelCheckpoint(filepath=checkpoint_path, \n",
        "                             save_weights_only=True,\n",
        "                             save_best_only=True, \n",
        "                             monitor='loss', \n",
        "                             verbose=1\n",
        "                            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wf4LPuyeHYD",
        "outputId": "76efaaa1-3ed2-4d89-e93b-38f21dfd91e2"
      },
      "source": [
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f'processing epoch: {epoch * 10 + 1}...')\n",
        "    seq2seq.fit([question_padded, answer_in_padded],\n",
        "                answer_out_one_hot,\n",
        "                epochs=10,\n",
        "                batch_size=BATCH_SIZE,\n",
        "                callbacks=[checkpoint]\n",
        "               )\n",
        "    # 랜덤한 샘플 번호 추출\n",
        "    samples = np.random.randint(DATA_LENGTH, size=SAMPLE_SIZE)\n",
        "\n",
        "    # 예측 성능 테스트\n",
        "    for idx in samples:\n",
        "        question_inputs = question_padded[idx]\n",
        "        # 문장 예측\n",
        "        results = make_prediction(seq2seq, np.expand_dims(question_inputs, 0))\n",
        "        \n",
        "        # 변환된 인덱스를 문장으로 변환\n",
        "        results = convert_index_to_text(results, END_TOKEN)\n",
        "        \n",
        "        print(f'Q: {questions[idx]}')\n",
        "        print(f'A: {results}\\n')\n",
        "        print()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing epoch: 1...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1650 - acc: 0.9623\n",
            "\n",
            "Epoch 00001: loss improved from inf to 0.16495, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1622 - acc: 0.9624\n",
            "\n",
            "Epoch 00002: loss improved from 0.16495 to 0.16224, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1570 - acc: 0.9635\n",
            "\n",
            "Epoch 00003: loss improved from 0.16224 to 0.15703, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1534 - acc: 0.9642\n",
            "\n",
            "Epoch 00004: loss improved from 0.15703 to 0.15336, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1537 - acc: 0.9638\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.15336\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1479 - acc: 0.9646\n",
            "\n",
            "Epoch 00006: loss improved from 0.15336 to 0.14791, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1442 - acc: 0.9658\n",
            "\n",
            "Epoch 00007: loss improved from 0.14791 to 0.14420, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1419 - acc: 0.9657\n",
            "\n",
            "Epoch 00008: loss improved from 0.14420 to 0.14193, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1397 - acc: 0.9658\n",
            "\n",
            "Epoch 00009: loss improved from 0.14193 to 0.13969, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1368 - acc: 0.9661\n",
            "\n",
            "Epoch 00010: loss improved from 0.13969 to 0.13682, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 기본 이 안 되어 있어\n",
            "A: 목 감기 오려나 봐요 \n",
            "\n",
            "\n",
            "Q: 독감 같아\n",
            "A: 저 는 마음 을 이어주는 위로 봇 입니다 \n",
            "\n",
            "\n",
            "Q: 걔 랑 같은 반 됐으면 좋겠다\n",
            "A: 당신 의 운 을 믿어 보세요 \n",
            "\n",
            "\n",
            "processing epoch: 11...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1352 - acc: 0.9663\n",
            "\n",
            "Epoch 00001: loss improved from 0.13682 to 0.13519, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1337 - acc: 0.9673\n",
            "\n",
            "Epoch 00002: loss improved from 0.13519 to 0.13374, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1309 - acc: 0.9673\n",
            "\n",
            "Epoch 00003: loss improved from 0.13374 to 0.13087, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.1308 - acc: 0.9674\n",
            "\n",
            "Epoch 00004: loss improved from 0.13087 to 0.13079, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1297 - acc: 0.9678\n",
            "\n",
            "Epoch 00005: loss improved from 0.13079 to 0.12970, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1301 - acc: 0.9673\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.12970\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1274 - acc: 0.9673\n",
            "\n",
            "Epoch 00007: loss improved from 0.12970 to 0.12743, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1239 - acc: 0.9684\n",
            "\n",
            "Epoch 00008: loss improved from 0.12743 to 0.12386, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.1226 - acc: 0.9685\n",
            "\n",
            "Epoch 00009: loss improved from 0.12386 to 0.12265, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1203 - acc: 0.9691\n",
            "\n",
            "Epoch 00010: loss improved from 0.12265 to 0.12028, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 나들이 를 가볼까\n",
            "A: 같이 가요 \n",
            "\n",
            "\n",
            "Q: 결혼 하면 좋을까\n",
            "A: 지금 도 그래요 \n",
            "\n",
            "\n",
            "Q: 누구세요\n",
            "A: 저 는 위로 해드리는 로봇 이에요 \n",
            "\n",
            "\n",
            "processing epoch: 21...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.1235 - acc: 0.9681\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.12028\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.1218 - acc: 0.9683\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.12028\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1179 - acc: 0.9693\n",
            "\n",
            "Epoch 00003: loss improved from 0.12028 to 0.11791, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1171 - acc: 0.9695\n",
            "\n",
            "Epoch 00004: loss improved from 0.11791 to 0.11714, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1172 - acc: 0.9694\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.11714\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1178 - acc: 0.9691\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.11714\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1126 - acc: 0.9704\n",
            "\n",
            "Epoch 00007: loss improved from 0.11714 to 0.11262, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1097 - acc: 0.9710\n",
            "\n",
            "Epoch 00008: loss improved from 0.11262 to 0.10969, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1120 - acc: 0.9706\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.10969\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1083 - acc: 0.9709\n",
            "\n",
            "Epoch 00010: loss improved from 0.10969 to 0.10829, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 계속 보고 싶어\n",
            "A: 보러 가세 요 \n",
            "\n",
            "\n",
            "Q: 놀이동산 가고 싶어\n",
            "A: 기분 이 나쁘셨나 봐요 \n",
            "\n",
            "\n",
            "Q: 감기 같 애\n",
            "A: 병원 가세 요 \n",
            "\n",
            "\n",
            "processing epoch: 31...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.1054 - acc: 0.9719\n",
            "\n",
            "Epoch 00001: loss improved from 0.10829 to 0.10537, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1065 - acc: 0.9713\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.10537\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1035 - acc: 0.9719\n",
            "\n",
            "Epoch 00003: loss improved from 0.10537 to 0.10351, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.1018 - acc: 0.9728\n",
            "\n",
            "Epoch 00004: loss improved from 0.10351 to 0.10183, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.1000 - acc: 0.9725\n",
            "\n",
            "Epoch 00005: loss improved from 0.10183 to 0.09996, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0988 - acc: 0.9730\n",
            "\n",
            "Epoch 00006: loss improved from 0.09996 to 0.09878, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0971 - acc: 0.9732\n",
            "\n",
            "Epoch 00007: loss improved from 0.09878 to 0.09713, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0959 - acc: 0.9735\n",
            "\n",
            "Epoch 00008: loss improved from 0.09713 to 0.09590, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0957 - acc: 0.9738\n",
            "\n",
            "Epoch 00009: loss improved from 0.09590 to 0.09570, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0928 - acc: 0.9750\n",
            "\n",
            "Epoch 00010: loss improved from 0.09570 to 0.09279, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 기프트 콘 으로 선물 받았어\n",
            "A: 좀 더 기다려주세요 \n",
            "\n",
            "\n",
            "Q: 내 친구 에게 내 험담 을 하다니\n",
            "A: 진짜 나빴네요 \n",
            "\n",
            "\n",
            "Q: 게임 때문 에 폰 이 점점 느려지는듯\n",
            "A: 정리 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 41...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0913 - acc: 0.9748\n",
            "\n",
            "Epoch 00001: loss improved from 0.09279 to 0.09134, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0886 - acc: 0.9756\n",
            "\n",
            "Epoch 00002: loss improved from 0.09134 to 0.08856, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0896 - acc: 0.9752\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.08856\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0885 - acc: 0.9757\n",
            "\n",
            "Epoch 00004: loss improved from 0.08856 to 0.08848, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0860 - acc: 0.9762\n",
            "\n",
            "Epoch 00005: loss improved from 0.08848 to 0.08597, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0859 - acc: 0.9760\n",
            "\n",
            "Epoch 00006: loss improved from 0.08597 to 0.08587, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0846 - acc: 0.9765\n",
            "\n",
            "Epoch 00007: loss improved from 0.08587 to 0.08456, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0814 - acc: 0.9777\n",
            "\n",
            "Epoch 00008: loss improved from 0.08456 to 0.08137, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0799 - acc: 0.9783\n",
            "\n",
            "Epoch 00009: loss improved from 0.08137 to 0.07989, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0779 - acc: 0.9785\n",
            "\n",
            "Epoch 00010: loss improved from 0.07989 to 0.07791, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 교통사고 당했어\n",
            "A: 보험 처리 하세요 \n",
            "\n",
            "\n",
            "Q: 말 하고 나니 후련하네여\n",
            "A: 돈 을 모아서 다른 곳 으로 이사 갈 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 논문 쓰기 힘들다\n",
            "A: 발 이 달렸나 봐요 \n",
            "\n",
            "\n",
            "processing epoch: 51...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0773 - acc: 0.9787\n",
            "\n",
            "Epoch 00001: loss improved from 0.07791 to 0.07732, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0756 - acc: 0.9790\n",
            "\n",
            "Epoch 00002: loss improved from 0.07732 to 0.07563, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0759 - acc: 0.9790\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.07563\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0724 - acc: 0.9803\n",
            "\n",
            "Epoch 00004: loss improved from 0.07563 to 0.07243, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0741 - acc: 0.9794\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.07243\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0713 - acc: 0.9806\n",
            "\n",
            "Epoch 00006: loss improved from 0.07243 to 0.07135, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0704 - acc: 0.9806\n",
            "\n",
            "Epoch 00007: loss improved from 0.07135 to 0.07045, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0673 - acc: 0.9811\n",
            "\n",
            "Epoch 00008: loss improved from 0.07045 to 0.06734, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0704 - acc: 0.9807\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.06734\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0676 - acc: 0.9816\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.06734\n",
            "Q: 똥마려\n",
            "A: 안된다고 하면 거짓말 이겠지요 \n",
            "\n",
            "\n",
            "Q: 거지 됐어\n",
            "A: 밥 사줄 친구 를 찾아 보세요 \n",
            "\n",
            "\n",
            "Q: 맨몸 으로 결혼 하기 가능\n",
            "A: 서로 마음 만 맞으면 가능해요 \n",
            "\n",
            "\n",
            "processing epoch: 61...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0648 - acc: 0.9825\n",
            "\n",
            "Epoch 00001: loss improved from 0.06734 to 0.06479, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0639 - acc: 0.9824\n",
            "\n",
            "Epoch 00002: loss improved from 0.06479 to 0.06394, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0598 - acc: 0.9842\n",
            "\n",
            "Epoch 00003: loss improved from 0.06394 to 0.05984, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0585 - acc: 0.9842\n",
            "\n",
            "Epoch 00004: loss improved from 0.05984 to 0.05855, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0580 - acc: 0.9844\n",
            "\n",
            "Epoch 00005: loss improved from 0.05855 to 0.05805, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0565 - acc: 0.9851\n",
            "\n",
            "Epoch 00006: loss improved from 0.05805 to 0.05649, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0543 - acc: 0.9856\n",
            "\n",
            "Epoch 00007: loss improved from 0.05649 to 0.05435, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0534 - acc: 0.9857\n",
            "\n",
            "Epoch 00008: loss improved from 0.05435 to 0.05338, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0523 - acc: 0.9866\n",
            "\n",
            "Epoch 00009: loss improved from 0.05338 to 0.05228, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0502 - acc: 0.9868\n",
            "\n",
            "Epoch 00010: loss improved from 0.05228 to 0.05020, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 면도 귀 찬 하\n",
            "A: 지저분해요 \n",
            "\n",
            "\n",
            "Q: 가상 화폐 쫄딱 망함\n",
            "A: 어서 잊고 새 출발 하세요 \n",
            "\n",
            "\n",
            "Q: 내 가 이상한 사람 같아\n",
            "A: 자신 의 독특함을 믿으세요 \n",
            "\n",
            "\n",
            "processing epoch: 71...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0495 - acc: 0.9869\n",
            "\n",
            "Epoch 00001: loss improved from 0.05020 to 0.04952, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0496 - acc: 0.9872\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.04952\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0467 - acc: 0.9882\n",
            "\n",
            "Epoch 00003: loss improved from 0.04952 to 0.04669, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0470 - acc: 0.9879\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.04669\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0450 - acc: 0.9885\n",
            "\n",
            "Epoch 00005: loss improved from 0.04669 to 0.04497, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0440 - acc: 0.9884\n",
            "\n",
            "Epoch 00006: loss improved from 0.04497 to 0.04400, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0456 - acc: 0.9888\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.04400\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0462 - acc: 0.9876\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.04400\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 19ms/step - loss: 0.0426 - acc: 0.9893\n",
            "\n",
            "Epoch 00009: loss improved from 0.04400 to 0.04259, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0423 - acc: 0.9892\n",
            "\n",
            "Epoch 00010: loss improved from 0.04259 to 0.04226, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 결혼 도 다 돈 이다\n",
            "A: 많이 들지만 줄일 수 있을 거 예요 \n",
            "\n",
            "\n",
            "Q: 넘어져서 발목 삔 거 같 애\n",
            "A: 꾸준히 치료 하세요 \n",
            "\n",
            "\n",
            "Q: 뭐 좀 물어 봐도 돼\n",
            "A: 무엇 이든 물어보세요 \n",
            "\n",
            "\n",
            "processing epoch: 81...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0388 - acc: 0.9907\n",
            "\n",
            "Epoch 00001: loss improved from 0.04226 to 0.03881, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0402 - acc: 0.9897\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.03881\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0375 - acc: 0.9909\n",
            "\n",
            "Epoch 00003: loss improved from 0.03881 to 0.03745, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0434 - acc: 0.9898\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.03745\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0392 - acc: 0.9903\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.03745\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0384 - acc: 0.9910\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.03745\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0354 - acc: 0.9915\n",
            "\n",
            "Epoch 00007: loss improved from 0.03745 to 0.03538, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0344 - acc: 0.9915\n",
            "\n",
            "Epoch 00008: loss improved from 0.03538 to 0.03438, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0343 - acc: 0.9916\n",
            "\n",
            "Epoch 00009: loss improved from 0.03438 to 0.03432, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0355 - acc: 0.9910\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.03432\n",
            "Q: 건강 이 최고 인 것 같아\n",
            "A: 가장 중요한 목표 네 요 \n",
            "\n",
            "\n",
            "Q: 단수\n",
            "A: 단수 가 되었는지 문의 해 보세요 \n",
            "\n",
            "\n",
            "Q: 뭐 입고 나가지\n",
            "A: 환해 보이는 옷 이 요 \n",
            "\n",
            "\n",
            "processing epoch: 91...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0319 - acc: 0.9922\n",
            "\n",
            "Epoch 00001: loss improved from 0.03432 to 0.03192, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0302 - acc: 0.9931\n",
            "\n",
            "Epoch 00002: loss improved from 0.03192 to 0.03024, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0303 - acc: 0.9927\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.03024\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0284 - acc: 0.9936\n",
            "\n",
            "Epoch 00004: loss improved from 0.03024 to 0.02839, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0273 - acc: 0.9941\n",
            "\n",
            "Epoch 00005: loss improved from 0.02839 to 0.02727, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0276 - acc: 0.9935\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.02727\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0278 - acc: 0.9938\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.02727\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0263 - acc: 0.9940\n",
            "\n",
            "Epoch 00008: loss improved from 0.02727 to 0.02629, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0273 - acc: 0.9936\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.02629\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0260 - acc: 0.9944\n",
            "\n",
            "Epoch 00010: loss improved from 0.02629 to 0.02596, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 뭐 하면서 노 는 게 좋을까\n",
            "A: 나 랑 같이 놀아요 \n",
            "\n",
            "\n",
            "Q: 공부 는 내 체질 이 아닌 것 같아\n",
            "A: 확신 이 없나 봐요 \n",
            "\n",
            "\n",
            "Q: 교회 가기 싫어\n",
            "A: 왜 그럴까 요 \n",
            "\n",
            "\n",
            "processing epoch: 101...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0246 - acc: 0.9945\n",
            "\n",
            "Epoch 00001: loss improved from 0.02596 to 0.02456, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0243 - acc: 0.9947\n",
            "\n",
            "Epoch 00002: loss improved from 0.02456 to 0.02432, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0277 - acc: 0.9937\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.02432\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0259 - acc: 0.9938\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.02432\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0237 - acc: 0.9947\n",
            "\n",
            "Epoch 00005: loss improved from 0.02432 to 0.02374, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0235 - acc: 0.9946\n",
            "\n",
            "Epoch 00006: loss improved from 0.02374 to 0.02350, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0240 - acc: 0.9944\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.02350\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0225 - acc: 0.9950\n",
            "\n",
            "Epoch 00008: loss improved from 0.02350 to 0.02255, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0216 - acc: 0.9955\n",
            "\n",
            "Epoch 00009: loss improved from 0.02255 to 0.02156, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0202 - acc: 0.9953\n",
            "\n",
            "Epoch 00010: loss improved from 0.02156 to 0.02025, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 길 에서 담배 피우는 사람 싫어\n",
            "A: 저 도 싫어요 \n",
            "\n",
            "\n",
            "Q: 나 폰 중독 인 거 같 애\n",
            "A: 잠깐 핸드폰 을 내려 두세요 \n",
            "\n",
            "\n",
            "Q: 나 회사 에서 인정받고 싶어\n",
            "A: 자기개발 을 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 111...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0195 - acc: 0.9958\n",
            "\n",
            "Epoch 00001: loss improved from 0.02025 to 0.01949, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0197 - acc: 0.9957\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.01949\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0196 - acc: 0.9960\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.01949\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0198 - acc: 0.9956\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.01949\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0193 - acc: 0.9959\n",
            "\n",
            "Epoch 00005: loss improved from 0.01949 to 0.01927, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0182 - acc: 0.9960\n",
            "\n",
            "Epoch 00006: loss improved from 0.01927 to 0.01823, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0179 - acc: 0.9961\n",
            "\n",
            "Epoch 00007: loss improved from 0.01823 to 0.01786, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0206 - acc: 0.9951\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.01786\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0186 - acc: 0.9957\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.01786\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0174 - acc: 0.9961\n",
            "\n",
            "Epoch 00010: loss improved from 0.01786 to 0.01735, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 마음 을 정리 하고 있는데 뜻밖 의 남자 가 카톡 을 했네\n",
            "A: 정리 하는데 도움 이 될 수도 있겠네요 \n",
            "\n",
            "\n",
            "Q: 기프트 콘 받았어\n",
            "A: 좋겠어요 \n",
            "\n",
            "\n",
            "Q: 공시 준비 하는데 힘들다\n",
            "A: 잘 하고 있어요 당당해지세요 \n",
            "\n",
            "\n",
            "processing epoch: 121...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.0160 - acc: 0.9966\n",
            "\n",
            "Epoch 00001: loss improved from 0.01735 to 0.01601, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0195 - acc: 0.9953\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.01601\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0166 - acc: 0.9963\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.01601\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0160 - acc: 0.9965\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.01601\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0143 - acc: 0.9971\n",
            "\n",
            "Epoch 00005: loss improved from 0.01601 to 0.01427, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0135 - acc: 0.9973\n",
            "\n",
            "Epoch 00006: loss improved from 0.01427 to 0.01352, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0136 - acc: 0.9972\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.01352\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0146 - acc: 0.9971\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.01352\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0132 - acc: 0.9973\n",
            "\n",
            "Epoch 00009: loss improved from 0.01352 to 0.01321, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0154 - acc: 0.9965\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.01321\n",
            "Q: 라면 먹을래\n",
            "A: 저 는 좋아요 \n",
            "\n",
            "\n",
            "Q: 문 콕 당했어\n",
            "A: 보험 처리 하세요 \n",
            "\n",
            "\n",
            "Q: 나 새 옷 샀다\n",
            "A: 꼬까 옷 개시 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 131...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.0132 - acc: 0.9973\n",
            "\n",
            "Epoch 00001: loss improved from 0.01321 to 0.01317, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0173 - acc: 0.9962\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.01317\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0188 - acc: 0.9959\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.01317\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0155 - acc: 0.9967\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.01317\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0165 - acc: 0.9962\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.01317\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0159 - acc: 0.9962\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.01317\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0129 - acc: 0.9975\n",
            "\n",
            "Epoch 00007: loss improved from 0.01317 to 0.01291, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0117 - acc: 0.9977\n",
            "\n",
            "Epoch 00008: loss improved from 0.01291 to 0.01175, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0114 - acc: 0.9977\n",
            "\n",
            "Epoch 00009: loss improved from 0.01175 to 0.01143, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0113 - acc: 0.9977\n",
            "\n",
            "Epoch 00010: loss improved from 0.01143 to 0.01127, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 기프트 콘 선물 해볼까\n",
            "A: 직접 주는 게 더 좋을 것 같아요 \n",
            "\n",
            "\n",
            "Q: 미팅 한다\n",
            "A: 성공 을 기원 합니다 \n",
            "\n",
            "\n",
            "Q: 물 끓여서 차 마셔야지\n",
            "A: 부지런하시네요 \n",
            "\n",
            "\n",
            "processing epoch: 141...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0110 - acc: 0.9979\n",
            "\n",
            "Epoch 00001: loss improved from 0.01127 to 0.01100, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0137 - acc: 0.9967\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.01100\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0110 - acc: 0.9978\n",
            "\n",
            "Epoch 00003: loss improved from 0.01100 to 0.01100, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0103 - acc: 0.9978\n",
            "\n",
            "Epoch 00004: loss improved from 0.01100 to 0.01032, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0117 - acc: 0.9975\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.01032\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0117 - acc: 0.9975\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.01032\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0107 - acc: 0.9979\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.01032\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0093 - acc: 0.9982\n",
            "\n",
            "Epoch 00008: loss improved from 0.01032 to 0.00926, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0107 - acc: 0.9981\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00926\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0095 - acc: 0.9980\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00926\n",
            "Q: 돈 이 안 모여\n",
            "A: 한 푼 두 푼 차곡차곡 \n",
            "\n",
            "\n",
            "Q: 교회 가기 싫어\n",
            "A: 왜 그럴까 요 \n",
            "\n",
            "\n",
            "Q: 다 들 행복한 거 같은데\n",
            "A: 남 들 이 당신 을 볼 때 도 그렇게 생각 할수있어요 \n",
            "\n",
            "\n",
            "processing epoch: 151...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0090 - acc: 0.9983\n",
            "\n",
            "Epoch 00001: loss improved from 0.00926 to 0.00899, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0097 - acc: 0.9979\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00899\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0091 - acc: 0.9982\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.00899\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0095 - acc: 0.9980\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.00899\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 2s 20ms/step - loss: 0.0126 - acc: 0.9974\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00899\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0114 - acc: 0.9975\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00899\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0114 - acc: 0.9974\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00899\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0174 - acc: 0.9964\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00899\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0109 - acc: 0.9978\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00899\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0101 - acc: 0.9976\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00899\n",
            "Q: 말 하지 말걸\n",
            "A: 말 조심하세요 \n",
            "\n",
            "\n",
            "Q: 내 배우자 는 어디 있을까\n",
            "A: 바로 옆 에 있을수도 있어요 \n",
            "\n",
            "\n",
            "Q: 면허 따야하나\n",
            "A: 운전 하는거 재미있어요 \n",
            "\n",
            "\n",
            "processing epoch: 161...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0123 - acc: 0.9974\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.00899\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0092 - acc: 0.9981\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00899\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0075 - acc: 0.9985\n",
            "\n",
            "Epoch 00003: loss improved from 0.00899 to 0.00751, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0074 - acc: 0.9985\n",
            "\n",
            "Epoch 00004: loss improved from 0.00751 to 0.00740, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0089 - acc: 0.9978\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00740\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0068 - acc: 0.9986\n",
            "\n",
            "Epoch 00006: loss improved from 0.00740 to 0.00681, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0064 - acc: 0.9988\n",
            "\n",
            "Epoch 00007: loss improved from 0.00681 to 0.00637, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0063 - acc: 0.9988\n",
            "\n",
            "Epoch 00008: loss improved from 0.00637 to 0.00629, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0062 - acc: 0.9987\n",
            "\n",
            "Epoch 00009: loss improved from 0.00629 to 0.00624, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0069 - acc: 0.9985\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00624\n",
            "Q: 바다 보러 가자고 해볼까\n",
            "A: 바다 는 사계절 내내 좋아요 \n",
            "\n",
            "\n",
            "Q: 겁난다\n",
            "A: 용기 내 보세요 \n",
            "\n",
            "\n",
            "Q: 먹고 나 니까 졸리 당\n",
            "A: 자연 의 이치 죠 안녕히 주무 세 요 \n",
            "\n",
            "\n",
            "processing epoch: 171...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.0094 - acc: 0.9977\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.00624\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0102 - acc: 0.9977\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00624\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0101 - acc: 0.9977\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.00624\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0083 - acc: 0.9982\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.00624\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0067 - acc: 0.9987\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00624\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0068 - acc: 0.9986\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00624\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 20ms/step - loss: 0.0082 - acc: 0.9983\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00624\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0079 - acc: 0.9982\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00624\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0075 - acc: 0.9983\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00624\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0076 - acc: 0.9983\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00624\n",
            "Q: 남자 가 화장 하는 거 어떻게 생각 해\n",
            "A: 적당히 하면 괜찮을거 같아요 \n",
            "\n",
            "\n",
            "Q: 면접 만 보면 떨려서 말 을 못 하겠어\n",
            "A: 회사 와 자신 에 대해 서 더 공부 해서 자신감 을 가져 보세요 \n",
            "\n",
            "\n",
            "Q: 나 두 잘 할거야\n",
            "A: 잘 하실 거 예요 \n",
            "\n",
            "\n",
            "processing epoch: 181...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 24ms/step - loss: 0.0070 - acc: 0.9984\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.00624\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0067 - acc: 0.9984\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00624\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0068 - acc: 0.9983\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.00624\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0062 - acc: 0.9986\n",
            "\n",
            "Epoch 00004: loss improved from 0.00624 to 0.00616, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0069 - acc: 0.9984\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00616\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0056 - acc: 0.9987\n",
            "\n",
            "Epoch 00006: loss improved from 0.00616 to 0.00559, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0081 - acc: 0.9981\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00559\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0095 - acc: 0.9981\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00559\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 23ms/step - loss: 0.0076 - acc: 0.9983\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00559\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0070 - acc: 0.9981\n",
            "\n",
            "Epoch 00010: loss did not improve from 0.00559\n",
            "Q: 단체 생활 해야 되는거 싫다\n",
            "A: 점점 적응 될 거 예요 \n",
            "\n",
            "\n",
            "Q: 목욕탕 가야 지\n",
            "A: 시원하게 씻고 오세요 \n",
            "\n",
            "\n",
            "Q: 돈 없어도 갈 수 있는 데이트 코스\n",
            "A: 등산 데이트 를 해보세요 \n",
            "\n",
            "\n",
            "processing epoch: 191...\n",
            "Epoch 1/10\n",
            "126/126 [==============================] - 3s 25ms/step - loss: 0.0090 - acc: 0.9982\n",
            "\n",
            "Epoch 00001: loss did not improve from 0.00559\n",
            "Epoch 2/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0062 - acc: 0.9987\n",
            "\n",
            "Epoch 00002: loss did not improve from 0.00559\n",
            "Epoch 3/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0074 - acc: 0.9982\n",
            "\n",
            "Epoch 00003: loss did not improve from 0.00559\n",
            "Epoch 4/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0066 - acc: 0.9984\n",
            "\n",
            "Epoch 00004: loss did not improve from 0.00559\n",
            "Epoch 5/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0075 - acc: 0.9982\n",
            "\n",
            "Epoch 00005: loss did not improve from 0.00559\n",
            "Epoch 6/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0093 - acc: 0.9979\n",
            "\n",
            "Epoch 00006: loss did not improve from 0.00559\n",
            "Epoch 7/10\n",
            "126/126 [==============================] - 3s 22ms/step - loss: 0.0074 - acc: 0.9981\n",
            "\n",
            "Epoch 00007: loss did not improve from 0.00559\n",
            "Epoch 8/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0057 - acc: 0.9988\n",
            "\n",
            "Epoch 00008: loss did not improve from 0.00559\n",
            "Epoch 9/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0059 - acc: 0.9987\n",
            "\n",
            "Epoch 00009: loss did not improve from 0.00559\n",
            "Epoch 10/10\n",
            "126/126 [==============================] - 3s 21ms/step - loss: 0.0053 - acc: 0.9989\n",
            "\n",
            "Epoch 00010: loss improved from 0.00559 to 0.00532, saving model to model/seq2seq-attention-checkpoint.ckpt\n",
            "Q: 시간 낭비 인데 자꾸 보게 됨\n",
            "A: 시간 을 정 하고 해보세요 \n",
            "\n",
            "\n",
            "Q: 드디어 데이트 인데 뭐 하지\n",
            "A: 야경 이 멋져요 야경 구경 가세 요 \n",
            "\n",
            "\n",
            "Q: 마음 을 보여 주고 싶어\n",
            "A: 그래서 표현 이 중요해요 \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eeaEokUFeP_v",
        "outputId": "f890b1ce-bf11-452a-9857-a3377b7203fe"
      },
      "source": [
        "while True:\n",
        "    user_input = input('<< 말을 걸어 보세요!\\n')\n",
        "    if user_input == 'q':\n",
        "        break\n",
        "    print('>> 챗봇 응답: {}'.format(run_chatbot(user_input)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<< 말을 걸어 보세요!\n",
            "오늘 날씨 어때?\n",
            ">> 챗봇 응답: 피해 를 안 준다면 무시 하세요 \n",
            "<< 말을 걸어 보세요!\n",
            "q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WwFVuD7ueTPB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}